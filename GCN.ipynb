{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre objectif dans ce Notebook est d'entrainer un GCN pour prédire les genres musicaux de prédilection d'un utilisateur en fonction de ses connexions dans le graphe. Nous utiliserons enusite les embeddings produit par le modèle pour déterminer des clusters et juger de leur qualité par rapport à d'autres approches de détection de communautés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par mettre le dataset des préférences deezer en Croatie dans un format utilisable par un réseau de neurone DGL (bibliothèque python pour les réseaux de neurones sur graphes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54573, 84)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparons les données sur les noeuds pour qu'elles soient utilisables par le modèle\n",
    "\n",
    "with open('deezer_clean_data/HR_genres.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# On crée un dictionnaire qui associe à chaque genre un numéro\n",
    "\n",
    "nodes = list(data.keys())\n",
    "genres = []\n",
    "\n",
    "for key, value in data.items():\n",
    "    for genre in value :\n",
    "        if genre not in genres:\n",
    "            genres.append(genre)\n",
    "\n",
    "# On crée un dictionnaire qui associe à chaque noeud un vecteur de 0 et de 1 en fonction de ses genres\n",
    "\n",
    "dico = {}\n",
    "\n",
    "for key, value in data.items():\n",
    "    vect = [0]*len(genres)\n",
    "    for genre in value:\n",
    "        vect[genres.index(genre)] = 1\n",
    "    dico[key] = vect\n",
    "\n",
    "# On ordonne les clés en ordre croissant \n",
    "\n",
    "dico = dict(sorted(dico.items()))\n",
    "\n",
    "# On transforme le dictionnaire en array numpy\n",
    "\n",
    "X = np.array(list(dico.values()))\n",
    "\n",
    "# On supprime les noeuds qui ne sont reliés avec aucun autre noeud\n",
    "\n",
    "edges_data = pd.read_csv(\"deezer_clean_data/HR_edges.csv\")\n",
    "\n",
    "# Assuming your CSV has two columns named 'source' and 'target' representing edges\n",
    "edges = [(row['node_1'], row['node_2']) for index, row in edges_data.iterrows()]\n",
    "\n",
    "# Construct the graph from the edge data\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Calculate the degree of each node\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "# Identify nodes with zero degree\n",
    "zero_degree_nodes = [node for node, degree in degrees.items() if degree == 0]\n",
    "\n",
    "print(zero_degree_nodes)\n",
    "\n",
    "#Remove zero degree nodes from X\n",
    "\n",
    "for i in zero_degree_nodes:\n",
    "    X = np.delete(X, i, 0)\n",
    "\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=54573, num_edges=552775,\n",
      "      ndata_schemes={'label': Scheme(shape=(84,), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import torch\n",
    "from dgl.data import DGLDataset\n",
    "\n",
    "\n",
    "class HRDeezerDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"HRDeezer\")\n",
    "\n",
    "    def process(self):\n",
    "        #nodes_data = pd.read_csv(\"/members.csv\")\n",
    "        edges_data = pd.read_csv(\"deezer_clean_data/HR_edges.csv\")\n",
    "        #node_features = torch.from_numpy(np.zeros((X.shape[0], 1)))\n",
    "        # node_labels = torch.from_numpy(\n",
    "        #     nodes_data[\"Club\"].astype(\"category\").cat.codes.to_numpy()\n",
    "        # )\n",
    "        node_labels = torch.from_numpy(X).float()\n",
    "        #edge_features = torch.from_numpy(edges_data[\"Weight\"].to_numpy())\n",
    "        edges_src = torch.from_numpy(edges_data[\"node_1\"].to_numpy())\n",
    "        edges_dst = torch.from_numpy(edges_data[\"node_2\"].to_numpy())\n",
    "\n",
    "        self.graph = dgl.graph(\n",
    "            (edges_src, edges_dst), num_nodes=X.shape[0]\n",
    "        )\n",
    "        #self.graph.ndata[\"feat\"] = node_features\n",
    "        self.graph.ndata[\"label\"] = node_labels\n",
    "        #self.graph.edata[\"weight\"] = None\n",
    "\n",
    "        # If your dataset is a node classification dataset, you will need to assign\n",
    "        # masks indicating whether a node belongs to training, validation, and test set.\n",
    "        n_nodes = X.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train : n_train + n_val] = True\n",
    "        test_mask[n_train + n_val :] = True\n",
    "        self.graph.ndata[\"train_mask\"] = train_mask\n",
    "        self.graph.ndata[\"val_mask\"] = val_mask\n",
    "        self.graph.ndata[\"test_mask\"] = test_mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def num_classes(self):\n",
    "        return X.shape[1]\n",
    "    \n",
    "    def num_nodes(self):\n",
    "        return X.shape[0]\n",
    "\n",
    "\n",
    "dataset = HRDeezerDataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous déclarons maintenant notre réseau. Nous commençons par un petit réseau contenant des couches convolutives pour graphes permettant le passage de messages entre les noeuds. \n",
    "Les noeuds n'ayant pas de features nous leur associons des embeddings que le réseau va pouvoir apprendre. Les embeddings sont intialement générés aléatoirement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_feats1, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        #On créé des embeddings pour les noeuds\n",
    "        self.node_embedding = nn.Embedding(num_nodes, h_feats1)\n",
    "        # Initialize the embeddings with small random values\n",
    "        nn.init.normal_(self.node_embedding.weight, std=4)\n",
    "        self.conv1 = GraphConv(h_feats1, h_feats1*2, weight=True)\n",
    "        self.conv2 = GraphConv(h_feats1*2, h_feats1*4, weight=True)\n",
    "        self.conv3 = GraphConv(h_feats1*4, h_feats1*6, weight=True)\n",
    "        self.linear = nn.Linear(h_feats1*6, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, g):\n",
    "        x = self.node_embedding.weight\n",
    "        x = self.conv1(g, x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(g, x)\n",
    "        x= F.relu(x)\n",
    "        x = self.conv3(g, x)\n",
    "        embs = self.linear(x)\n",
    "        x = self.sigmoid(embs)\n",
    "        return x, embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On déclare la fonction d'entrainement du réseau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    #features = g.ndata[\"feat\"]\n",
    "    labels = g.ndata[\"label\"]\n",
    "    train_mask = g.ndata[\"train_mask\"]\n",
    "    val_mask = g.ndata[\"val_mask\"]\n",
    "    test_mask = g.ndata[\"test_mask\"]\n",
    "    for e in range(100):\n",
    "        # Forward\n",
    "        logits, embs = model(g) \n",
    "        \n",
    "        # Compute prediction\n",
    "        pred = logits > 0.5\n",
    "        \n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        \n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        # Check for identical rows\n",
    "        identical_rows = torch.all(torch.eq(pred[train_mask], labels[train_mask]), dim=1)\n",
    "        num_identical_rows = torch.sum(identical_rows).item()\n",
    "        train_acc = num_identical_rows / train_mask.sum().item()\n",
    "\n",
    "        identical_rows = torch.all(torch.eq(pred[val_mask], labels[val_mask]), dim=1)\n",
    "        num_identical_rows = torch.sum(identical_rows).item()\n",
    "        val_acc = num_identical_rows / val_mask.sum().item()\n",
    "\n",
    "        identical_rows = torch.all(torch.eq(pred[test_mask], labels[test_mask]), dim=1)\n",
    "        num_identical_rows = torch.sum(identical_rows).item()\n",
    "        test_acc = num_identical_rows / test_mask.sum().item()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print(\n",
    "                f\"In epoch {e}, loss: {loss:.3f}, train acc: {train_acc:.3f} val acc: {val_acc:.3f} (best {best_val_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (node_embedding): Embedding(54573, 32)\n",
      "  (conv1): GraphConv(in=32, out=64, normalization=both, activation=None)\n",
      "  (conv2): GraphConv(in=64, out=128, normalization=both, activation=None)\n",
      "  (conv3): GraphConv(in=128, out=192, normalization=both, activation=None)\n",
      "  (linear): Linear(in_features=192, out_features=84, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset = HRDeezerDataset()\n",
    "task = 'node'\n",
    "model = GCN(dataset.num_nodes(), 32, dataset.num_classes())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2(graph, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème : la matrice est trop parsemée et l'entrainement pousse le réseau à descendre tous les poids à 0. En effet, les utilisateurs n'ont en général que 2 ou 3 genres musicaux associés sur 84 ce qui fait que les 0 sont largement majoritaires dans les tenseurs associés et que diminuer la loss revient à descendre les poids à 0. C'est ce qu'on observe pendant l'entrainement. Notre réseau est surement trop petit pour saisir la complexité des données. On essaye de faire une prédiction multi-label multi-classe sur 84 classes, ce qui représente une tâche compliquée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des embeddings sortants du réseau entrainé ##\n",
    "\n",
    "On va chercher à représenter les embeddings générés par le réseau au cours de l'entrainement. On a pas trop d'espoir de voir une séparation claire étant donnée la qualité des prédictions du réseau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Une fonction pour afficher les représentation en dimensions réduites\n",
    "\n",
    "def plot_scatter(embeddings, labels, title):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='viridis', s=10)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "#On représente les embeddings en 2D\n",
    "\n",
    "n_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54573, 32])\n",
      "torch.Size([54573, 84])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embs = []\n",
    "\n",
    "\n",
    "pred, emb = model(graph)\n",
    "\n",
    "print(emb.shape)\n",
    "\n",
    "\n",
    "emb_np = pred.detach().numpy()\n",
    "\n",
    "print(pred.shape)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(emb_np)\n",
    "\n",
    "#We plot the t-SNE representation of the embeddings\n",
    "\n",
    "plot_scatter(tsne_result, labels=None, title='t-SNE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "con_rais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
